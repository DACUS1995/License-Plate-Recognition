{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms as T\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nRunning training on [cuda]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import string\n",
    "from typing import Tuple\n",
    "import datetime\n",
    "import copy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running training on [{device}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "WORKERS = 1\n",
    "EPOCHS = 10\n",
    "MAX_WORD_LENGTH = 10\n",
    "ALPHABET = string.ascii_letters + string.digits + \"_\" #blank char for CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, root_path, type=\"train\"):\n",
    "\t\tself.root_path = root_path\n",
    "\t\tself.type = type\n",
    "\t\tself.images_paths = list(pathlib.Path(self.root_path + \"./images\").glob('*.png'))\n",
    "\t\tself.transforms = {\n",
    "\t\t\t'train' : T.Compose([\n",
    "\t\t\t\tT.Resize((40,200)),\n",
    "\t\t\t\tT.RandomRotation(20),\n",
    "\t\t\t\tT.GaussianBlur(3),\n",
    "\t\t\t\tT.ToTensor()\n",
    "\t\t\t]),\n",
    "\t\t\t'valid' : T.Compose([\n",
    "\t\t\t\tT.ToTensor()\n",
    "\t\t\t])\n",
    "\t\t}\n",
    "\t\tglobal ALPHABET\n",
    "\t\tself.alphabet = ALPHABET\n",
    "\t\tself.alphabet_size = len(self.alphabet)\n",
    "\t\tprint(f\"Alphabet size: {self.alphabet_size}\")\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage_path = self.images_paths[idx]\n",
    "\t\tsample_name = str(image_path).split(os.sep)[-1].split(\".\")[0]\n",
    "\t\ttext_path = self.root_path + \"/transcripts/\" + str(int(sample_name) + 1) + \".txt\"\n",
    "\n",
    "\t\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\t\twith open(text_path) as f:\n",
    "\t\t\ttext = f.read()\n",
    "\n",
    "\t\timage = self.transforms[self.type](image)\n",
    "\t\ttext_tensor = self.wordToTensor(text)\n",
    "\t\treturn image, (text_tensor, len(text), text)\n",
    "\t\t\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images_paths)\n",
    "\n",
    "\tdef letterToIndex(self, letter):\n",
    "\t\treturn self.alphabet.find(letter)\n",
    "\n",
    "\tdef letterToTensor(self, letter):\n",
    "\t\ttensor = torch.zeros(1, n_letters)\n",
    "\t\ttensor[0][letterToIndex(letter)] = 1\n",
    "\t\treturn tensor\n",
    "\n",
    "\tdef wordToTensor(self, word):\n",
    "\t\t# tensor = torch.zeros(MAX_WORD_LENGTH, self.alphabet_size)\n",
    "\t\t# for li, letter in enumerate(word):\n",
    "\t\t# \ttensor[li][self.letterToIndex(letter)] = 1\n",
    "\n",
    "\t\ttensor = torch.zeros(MAX_WORD_LENGTH)\n",
    "\t\tfor li, letter in enumerate(word):\n",
    "\t\t\ttensor[li] = self.letterToIndex(letter)\n",
    "\t\treturn tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "\tdef __init__(self, run_config):\n",
    "\t\tself._training_dataset = None\n",
    "\t\tself._validation_dataset = None\n",
    "\t\tself._run_config = run_config\n",
    "\n",
    "\t\tself._load_datasets()\n",
    "\t\t\n",
    "\tdef _load_datasets(self):\n",
    "\t\tself._training_dataset = CustomDataset(\"dataset/training\")\n",
    "\t\tself._validation_dataset = CustomDataset(\"dataset/validation\")\n",
    "\n",
    "\tdef get_data_loaders(self) -> Tuple[DataLoader]:\n",
    "\t\treturn (\n",
    "\t\t\tDataLoader(self._training_dataset, batch_size=self._run_config[\"batch_size\"], shuffle=True, pin_memory=True, drop_last=True), \n",
    "\t\t\tDataLoader(self._validation_dataset, batch_size=self._run_config[\"batch_size\"], shuffle=True, pin_memory=True, drop_last=True)\n",
    "\t\t)\n",
    "\n",
    "\tdef get_datasets(self) -> Tuple[Dataset]:\n",
    "\t\treturn self._training_dataset, self._validation_dataset\n",
    "\n",
    "\tdef get_datasets_sizes(self) -> Tuple[int]:\n",
    "\t\treturn len(self._training_dataset), len(self._validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alphabet size: 63\n",
      "Alphabet size: 63\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(run_config = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"workers\": WORKERS\n",
    "})\n",
    "train_loader, validation_loader = data_handler.get_data_loaders()\n",
    "training_dataset_size, validation_dataset_size = data_handler.get_datasets_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscribeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TranscribeModel, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels=3, out_channels=16, kernel_size=2, padding=1),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2),\n",
    "\t\t\tnn.BatchNorm2d(16),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\t\t\tnn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, padding=1),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2),\n",
    "\t\t\tnn.BatchNorm2d(32),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True)\n",
    "\t\t)\n",
    "\n",
    "        self.rnn_block1 = nn.Sequential(\n",
    "            nn.LSTM(input_size=320, hidden_size=62, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv_block1(input)\n",
    "        # print(out.shape) #[128, 32, 10, 50]\n",
    "        out = out.permute([0, 3, 2, 1])\n",
    "        # (B, S, ) \n",
    "        out = out.reshape(out.size(0), out.size(1), -1)\n",
    "\n",
    "        out, (_, _) = self.rnn_block1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TranscribeModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (rnn_block1): Sequential(\n",
       "    (0): LSTM(320, 62, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=2)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "model = TranscribeModel()\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001, \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-08, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "loss_criterion = nn.CTCLoss(blank=len(ALPHABET)-1)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=10, epochs=EPOCHS,anneal_strategy='linear')\n",
    "\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorToWord(tensor):\n",
    "\ttensor = tensor[..., :62]\n",
    "\tindices = torch.argmax(tensor, dim=2).tolist()\n",
    "\t\n",
    "\twords = []\n",
    "\tfor batch_idx in range(BATCH_SIZE):\n",
    "\t\tcur_word_indices = indices[batch_idx]\n",
    "\t\tcur_word = []\n",
    "\t\tlast_letter = None\n",
    "\t\tfor idx in range(50):\n",
    "\t\t\tif ALPHABET[idx] == ALPHABET[-1]:\n",
    "\t\t\t\tlast_letter = None\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\tif last_letter == None or (last_letter is not None and last_letter != cur_word_indices[idx]):\n",
    "\t\t\t\t\tcur_word.append(ALPHABET[cur_word_indices[idx]])\n",
    "\t\t\t\t\tlast_letter = cur_word_indices[idx]\n",
    "\t\t\t\t\tcontinue\n",
    "\t\twords.append(\"\".join(cur_word))\n",
    "\treturn words\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([10,  9,  5,  2,  5,  7,  4,  1,  3,  9,  9,  1,  4,  4,  5,  6,  2,  9,\n         6, 10,  9,  8, 10,  8,  4,  5, 10,  2,  1,  3,  8,  8,  5,  8,  5,  3,\n         9, 10,  2,  4,  5,  5,  9,  4,  1,  8,  3,  4, 10,  8,  6,  2,  7,  6,\n         7,  6,  1,  2,  3,  1,  4,  7,  2,  1,  1, 10, 10,  5,  2,  8,  2,  7,\n         9,  5,  8,  4,  7,  5, 10, 10,  3,  9,  4, 10,  4,  6,  6,  1,  4,  6,\n         9,  7,  4,  3,  8, 10,  8,  5,  6, 10,  4,  1,  7,  9,  3,  3,  8,  6,\n         1,  4,  4,  4,  7,  8,  5,  4, 10,  9,  5,  5,  5,  9,  4,  4,  6,  1,\n         7, 10])\n"
     ]
    }
   ],
   "source": [
    "image, (label_tensor, text_length, label_text) = iter(train_loader).next()\n",
    "image, label_tensor = image.to(device), label_tensor.to(device)\n",
    "output = model(image)\n",
    "print(text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 124])"
      ]
     },
     "metadata": {},
     "execution_count": 93
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1jHUlUrjw',\n",
       " 'SrTrHrTrHrjTrj',\n",
       " 'ArUrjNwM',\n",
       " 'ATwjHr',\n",
       " 'AUrFrFrFjHjHrjNw',\n",
       " 'HUwUwjTr',\n",
       " 'jrjrtrlHUrcrTwDZDM',\n",
       " 'SHrk',\n",
       " 'AwjsHr',\n",
       " 'jAHwHjr',\n",
       " 'AwHwjUr',\n",
       " 'jwFUwTsHr',\n",
       " 'TjwjwFjUFrHrTr',\n",
       " 'SfwAHwrterTwM6j',\n",
       " 'rHrTwM6j6w',\n",
       " 'TjUwHrF',\n",
       " 'AjwjKrHr',\n",
       " 'STHrjrjrj',\n",
       " 'hHFU6wjHr',\n",
       " 'hHFjFHwMFjFSruerSFJMr',\n",
       " 'ArjwM',\n",
       " 'rjwM6jU6w',\n",
       " 'rfwAHwrjrjwZDM6juB',\n",
       " 'jwHwjwHjFjTHTr',\n",
       " '6rHrjrj',\n",
       " '6THTFArATrjHr',\n",
       " 'ASrjrjwM',\n",
       " 'jHFUwMsHr',\n",
       " 'AjSjTKr',\n",
       " 'AjwjKHr',\n",
       " 'AHwjFjFjFjTr',\n",
       " '6UwUHTr',\n",
       " 'AwrwrSrFrjrTwM46j',\n",
       " 'GUrjw',\n",
       " 'AHjHjKFUwjUjFjHrTrTrTr',\n",
       " 'AjTr',\n",
       " 'TwMFMTFHrHUrHur',\n",
       " 'Hj6wHwjHr',\n",
       " 'SHrTwM46wj',\n",
       " 'rfHrTwM6j6w',\n",
       " 'jHwjTHr',\n",
       " 'TjU6UwMHrF',\n",
       " 'SrHjHrjrjUVjUjN',\n",
       " 'AUrjNwM',\n",
       " 'AjwHwjHr',\n",
       " 'ASrjwM46w6',\n",
       " '6SUwHwjsHr',\n",
       " 'jwFUjwTsHr',\n",
       " 'Arjw',\n",
       " 'HUj6FwjSjrTSrMrF',\n",
       " 'jHBFjFwHwqSGHrJjrSFJr',\n",
       " 'GUrjNwM',\n",
       " 'jFUFwFTHTHr',\n",
       " 'ArArTrHrNTw',\n",
       " 'jU4wjHrHrF',\n",
       " 'AjwjVjVjTrHr',\n",
       " 'GTqHqHAwyKTwTUHrvjr',\n",
       " 'AjwjKHr',\n",
       " 'THjFUFwTsHrF',\n",
       " 'jAwjHsHr',\n",
       " 'SrTwM46j',\n",
       " 'ArjrjNwM',\n",
       " 'jF6jFw4AwyTeHrvjrSrfJF',\n",
       " 'ArjNwM',\n",
       " '6THr',\n",
       " 'AjFjrjrjrjr',\n",
       " 'GrTrjwM',\n",
       " 'SHrTwM46j',\n",
       " 'HUSwsHr',\n",
       " 'jwV6FKjFjFjreTr',\n",
       " 'STHrj',\n",
       " 'ASrjrjNwM',\n",
       " 'jwFUwjMjHr',\n",
       " 'ArFjFjHrjNw',\n",
       " '6jFrjrjrjrjrjrjeHr',\n",
       " 'AUrjNwM',\n",
       " 'H6SjFwTwMwjFjrvjrjfF',\n",
       " 'STrj',\n",
       " '6wHwjHr',\n",
       " 'SfwAHrjwM6j',\n",
       " 'hFjFwHwAwMKTKHrJjrFJFr',\n",
       " 'Arjw',\n",
       " 'ArHrTwM46wj',\n",
       " 'AjwjrTr',\n",
       " 'jHwjwjwFjFjHKHr',\n",
       " 'ArMwMrjNwM',\n",
       " 'STrTrTjTHrTVT',\n",
       " 'SfJAHjt2rTDM46jq',\n",
       " 'ASrjNwM',\n",
       " 'ArUlUrHjHrjNwM',\n",
       " 'jwFUwjTr',\n",
       " 'ArHererjrTVwDjDZM',\n",
       " 'HUjwSKHrMrF',\n",
       " 'ArTKswDO',\n",
       " 'SrFrTrHrFrFrjrArArjrTVw',\n",
       " 'ASrjwM',\n",
       " 'HjU6wFwBHjHr',\n",
       " 'ASrjNwM',\n",
       " 'AjArTrjTHTHr',\n",
       " 'SjrjrjrjrjrjrjrjTHr',\n",
       " 'ASAHrTwM6j',\n",
       " 'HUwsHr',\n",
       " 'AjwjFjTrTr',\n",
       " 'HjUwMwjHTrF',\n",
       " 'SrHrTwM46jU',\n",
       " 'AUrjNwM',\n",
       " 'AwjrTr',\n",
       " 'jwHwFjHTHr',\n",
       " 'SuJkAHrjt2VrRcHrKTwDZDM46j',\n",
       " 'AjwHwjKHr',\n",
       " 'TwdwMFwSUHrvjr',\n",
       " 'EwjsHr',\n",
       " 'ArHrjNwM',\n",
       " 'jwjUjTHr',\n",
       " 'AjTr',\n",
       " 'AjwjHjwjwFjKHr',\n",
       " 'ArTrHjw',\n",
       " 'SrTrTeTeHeHrTrjTVT',\n",
       " '6jrjHr',\n",
       " 'AwHwjwjHr',\n",
       " 'HU6UwjsHr',\n",
       " 'AHrjwM46wj',\n",
       " 'GSrjNwM',\n",
       " 'ArerjNwM',\n",
       " 'jwHwHwjwFjKHr',\n",
       " 'SAwHrTwZM6j',\n",
       " 'ASrHrNwM46',\n",
       " 'ATHwjr']"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "output.shape\n",
    "out = output[..., :62]\n",
    "# torch.argmax(out, dim=2)\n",
    "\n",
    "tensorToWord(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/10\n----------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TranscribeModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (rnn_block1): Sequential(\n",
       "    (0): LSTM(320, 62, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=2)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 94
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [1] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc68fd4835724535afc155011ed18f9b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inf\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "nan\n",
      "128\n",
      "\n",
      "Training step => Loss: nan\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TranscribeModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (rnn_block1): Sequential(\n",
       "    (0): LSTM(320, 62, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=2)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 94
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation step => Loss: nan\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './checkpoints/ckp.pt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-1f037ec74999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;34m\"model_state_dict\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;34m\"optimizer_state_dict\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \t}, \"./checkpoints/ckp.pt\")\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints/ckp.pt'"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\tprint('Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "\tprint('-' * 10)\n",
    "\n",
    "\t########### Training step ###########\n",
    "\tmodel.train()\n",
    "\ttraining_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\t\t\t\n",
    "\tfor i, data in enumerate(tqdm(train_loader, desc=f\"Epoch [{epoch + 1}] progress\")):\n",
    "\n",
    "\t\tx_batch, (label_batch, label_length, label_text) = data\n",
    "\t\tx_batch, label_batch, label_length = x_batch.to(device), label_batch.to(device), label_length.to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(x_batch)\n",
    "\t\toutputs_permuted = outputs.permute((1, 0, 2))\n",
    "\n",
    "\t\tloss = loss_criterion(outputs_permuted, label_batch, torch.full((BATCH_SIZE,), MAX_WORD_LENGTH), label_length)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\t# statistics\n",
    "\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\trunning_corrects += torch.tensor([0])\n",
    "\t\t# running_corrects += torch.sum(preds == label_text)\n",
    "\t\ttraining_loss.append(loss.item())\n",
    "\n",
    "\tepoch_loss = running_loss / training_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / training_dataset_size\n",
    "\n",
    "\t# tensorboard logging\n",
    "\twriter.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "\n",
    "\tprint('Training step => Loss: {:.4f}'.format(\n",
    "\t\tepoch_loss\n",
    "\t))\n",
    "\n",
    "\tscheduler.step(epoch_acc)\n",
    "\n",
    "\n",
    "\t########### Validation step ###########\n",
    "\tmodel.eval()\n",
    "\tvalidation_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\n",
    "\tfor i, data in enumerate(validation_loader):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tx_batch, (label_batch, label_length, label_text) = data\n",
    "\t\t\tx_batch, label_batch, label_length = x_batch.to(device), label_batch.to(device), label_length.to(device)\n",
    "\n",
    "\t\t\toutputs = model(x_batch)\n",
    "\t\t\toutputs = outputs.permute((1, 0, 2))\n",
    "\t\t\tloss = loss_criterion(outputs, label_batch, torch.full((BATCH_SIZE,), MAX_WORD_LENGTH), label_length)\n",
    "\n",
    "\t\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\t\trunning_corrects += torch.tensor([0])\n",
    "\t\t\t# running_corrects += torch.sum(preds == label_batch.detach())\n",
    "\t\t\tvalidation_loss.append(loss.item())\n",
    "\t\t\t\n",
    "\tepoch_loss = running_loss / validation_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / validation_dataset_size\n",
    "\n",
    "\t# tensorboard logging\n",
    "\twriter.add_scalar(\"Loss/validation\", epoch_loss, epoch)\n",
    "\n",
    "\tprint('Evaluation step => Loss: {:.4f}'.format(\n",
    "\t\tepoch_loss\n",
    "\t))\n",
    "\tbest_acc = 0\n",
    "\t#Save the best model based on accuracy\n",
    "\tif True:\n",
    "\t\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\t#Checkpoint\n",
    "\ttorch.save({\n",
    "\t\t\"epoch\": epoch,\n",
    "\t\t\"model_state_dict\": model.state_dict(),\n",
    "\t\t\"optimizer_state_dict\": optimizer.state_dict()\n",
    "\t}, \"./checkpoints/ckp.pt\")\n",
    "\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "\ttime_elapsed // 60, time_elapsed % 60\n",
    "))\n",
    "print('Best (so far) validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "print('-' * 10)\n",
    "print('### Final results ###\\n')\n",
    "print('Best validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c67d8e0382c833d1d7ff2aaf3d36bcbf66720dd18abf32e81be90b4a114af6"
    }
   }
  },
  "language_info": {
   "name": "",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}