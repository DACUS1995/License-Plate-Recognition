{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms as T\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running training on [cuda]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import string\n",
    "from typing import Tuple\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running training on [{device}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "WORKERS = 1\n",
    "EPOCHS = 10\n",
    "MAX_WORD_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, root_path, type=\"train\"):\n",
    "\t\tself.root_path = root_path\n",
    "\t\tself.type = type\n",
    "\t\tself.images_paths = list(pathlib.Path(self.root_path + \"./images\").glob('*.png'))\n",
    "\t\tself.transforms = {\n",
    "\t\t\t'train' : T.Compose([\n",
    "\t\t\t\tT.Resize((40,200)),\n",
    "\t\t\t\tT.RandomRotation(20),\n",
    "\t\t\t\tT.GaussianBlur(3),\n",
    "\t\t\t\tT.ToTensor()\n",
    "\t\t\t]),\n",
    "\t\t\t'valid' : T.Compose([\n",
    "\t\t\t\tT.ToTensor()\n",
    "\t\t\t])\n",
    "\t\t}\n",
    "\t\tself.alphabet = string.ascii_letters + string.digits\n",
    "\t\tself.alphabet_size = len(self.alphabet)\n",
    "\t\tprint(f\"Alphabet size: {self.alphabet_size}\")\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage_path = self.images_paths[idx]\n",
    "\t\tsample_name = str(image_path).split(os.sep)[-1].split(\".\")[0]\n",
    "\t\ttext_path = self.root_path + \"/transcripts/\" + sample_name + \".txt\"\n",
    "\n",
    "\t\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\t\twith open(text_path) as f:\n",
    "\t\t\ttext = f.read()\n",
    "\n",
    "\t\timage = self.transforms[self.type](image)\n",
    "\t\ttext = self.wordToTensor(text)\n",
    "\t\treturn image, text\n",
    "\t\t\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images_paths)\n",
    "\n",
    "\tdef letterToIndex(self, letter):\n",
    "\t\treturn self.alphabet.find(letter)\n",
    "\n",
    "\tdef letterToTensor(self, letter):\n",
    "\t\ttensor = torch.zeros(1, n_letters)\n",
    "\t\ttensor[0][letterToIndex(letter)] = 1\n",
    "\t\treturn tensor\n",
    "\n",
    "\tdef wordToTensor(self, word):\n",
    "\t\ttensor = torch.zeros(MAX_WORD_LENGTH, self.alphabet_size)\n",
    "\t\tfor li, letter in enumerate(word):\n",
    "\t\t\ttensor[li][self.letterToIndex(letter)] = 1\n",
    "\t\treturn tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "\tdef __init__(self, run_config):\n",
    "\t\tself._training_dataset = None\n",
    "\t\tself._validation_dataset = None\n",
    "\t\tself._run_config = run_config\n",
    "\n",
    "\t\tself._load_datasets()\n",
    "\t\t\n",
    "\tdef _load_datasets(self):\n",
    "\t\tself._training_dataset = CustomDataset(\"dataset/training\")\n",
    "\t\tself._validation_dataset = CustomDataset(\"dataset/validation\")\n",
    "\n",
    "\tdef get_data_loaders(self) -> Tuple[DataLoader]:\n",
    "\t\treturn (\n",
    "\t\t\tDataLoader(self._training_dataset, batch_size=self._run_config[\"batch_size\"], shuffle=True, pin_memory=True), \n",
    "\t\t\tDataLoader(self._validation_dataset, batch_size=self._run_config[\"batch_size\"], shuffle=True, pin_memory=True)\n",
    "\t\t)\n",
    "\n",
    "\tdef get_datasets(self) -> Tuple[Dataset]:\n",
    "\t\treturn self._training_dataset, self._validation_dataset\n",
    "\n",
    "\tdef get_datasets_sizes(self) -> Tuple[int]:\n",
    "\t\treturn len(self._training_dataset), len(self._validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alphabet size: 62\nAlphabet size: 62\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(run_config = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"workers\": WORKERS\n",
    "})\n",
    "train_loader, validation_loader = data_handler.get_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscribeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TranscribeModel, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels=3, out_channels=16, kernel_size=2, padding=1),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2),\n",
    "\t\t\tnn.BatchNorm2d(16),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\t\t\tnn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, padding=1),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2),\n",
    "\t\t\tnn.BatchNorm2d(32),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True)\n",
    "\t\t)\n",
    "\n",
    "        self.rnn_block1 = nn.Sequential(\n",
    "            nn.LSTM(input_size=320, hidden_size=62, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv_block1(input)\n",
    "        # print(out.shape) #[128, 32, 10, 50]\n",
    "        out = out.permute([0, 3, 2, 1])\n",
    "        # (B, S, ) \n",
    "        out = out.reshape(out.size(0), out.size(1), -1)\n",
    "\n",
    "        out, (_, _) = self.rnn_block1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TranscribeModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (rnn_block1): Sequential(\n",
       "    (0): LSTM(320, 62, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=2)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "model = TranscribeModel()\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001, \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-08, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "loss_criterion = nn.CTCLoss()\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=10, epochs=EPOCHS,anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 32, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "image, text = iter(train_loader).next()\n",
    "image, text = image.to(device), text.to(device)\n",
    "output = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 124])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[26, 31, 49,  ..., 13, 17, 12],\n",
       "        [23, 24,  5,  ..., 13, 10,  9],\n",
       "        [24, 24,  2,  ...,  8, 17,  8],\n",
       "        ...,\n",
       "        [25, 26,  7,  ..., 33, 19, 12],\n",
       "        [32, 31, 43,  ...,  5, 40, 21],\n",
       "        [37, 16, 42,  ..., 22, 23, 44]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "output.shape\n",
    "out = output[..., :62]\n",
    "out = torch.argmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorToWord(tensor):\n",
    "\ttensor = tensor[..., :62]\n",
    "\tindices = torch.argmax(tensor, dim=1).tolist()\n",
    "\t\n",
    "\twords = []\n",
    "    for batch_idx in range(BATCH_SIZE):\n",
    "        cur_word_indices = indices[batch_idx]\n",
    "        cur_word = []\n",
    "        last_letter = None\n",
    "        for idx in range(50):\n",
    "            if alphabet[idx] == blank:\n",
    "                last_letter = None\n",
    "                continue\n",
    "            else:\n",
    "                if last_letter == None or (last_letter is not None and last_letter != cur_word_indices[idx]):\n",
    "                    cur_word.append(alphabet[cur_word_indices[idx]])\n",
    "                    last_letter = cur_word_indices[idx]\n",
    "                    continue\n",
    "        words.append(cur_word)\n",
    "    return words\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\tprint('Epoch {}/{}'.format(epoch, run.num_epochs))\n",
    "\tprint('-' * 10)\n",
    "\n",
    "\t########### Training step ###########\n",
    "\tmodel.train()\n",
    "\ttraining_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\t\t\t\n",
    "\tfor i, data in enumerate(tqdm(train_loader, desc=f\"Epoch [{epoch + 1}] progress\")):\n",
    "\n",
    "\t\tx_batch, label_batch = data\n",
    "\t\tx_batch, label_batch = x_batch.to(device), label_batch.to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(x_batch)\n",
    "\t\t_, preds = torch.max(outputs, 1)\n",
    "\n",
    "\t\tloss = loss_criterion(outputs, label_batch)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\t# statistics\n",
    "\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\trunning_corrects += torch.sum(preds == label_batch.detach())\n",
    "\t\ttraining_loss.append(loss.item())\n",
    "\n",
    "\tepoch_loss = running_loss / training_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / training_dataset_size\n",
    "\n",
    "\t# tensorboard logging\n",
    "\twriter.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "\twriter.add_scalar(\"Accuracy/train\", epoch_acc, epoch)\n",
    "\n",
    "\tprint('Training step => Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "\t\tepoch_loss, epoch_acc\n",
    "\t))\n",
    "\n",
    "\tscheduler.step(epoch_acc)\n",
    "\n",
    "\n",
    "\t########### Validation step ###########\n",
    "\tmodel.eval()\n",
    "\tvalidation_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\n",
    "\tfor i, data in enumerate(validation_loader):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tx_batch, label_batch = data\n",
    "\t\t\tx_batch, label_batch = x_batch.to(device), label_batch.to(device)\n",
    "\n",
    "\t\t\toutputs = model(x_batch)\n",
    "\t\t\t_, preds = torch.max(outputs, 1)\n",
    "\t\t\tloss = loss_criterion(...)\n",
    "\n",
    "\t\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\t\trunning_corrects += torch.sum(preds == label_batch.detach())\n",
    "\t\t\tvalidation_loss.append(loss.item())\n",
    "\t\t\t\n",
    "\tepoch_loss = running_loss / validation_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / validation_dataset_size\n",
    "\n",
    "\t# tensorboard logging\n",
    "\twriter.add_scalar(\"Loss/validation\", epoch_loss, epoch)\n",
    "\twriter.add_scalar(\"Accuracy/validation\", epoch_acc, epoch)\n",
    "\n",
    "\tprint('Evaluation step => Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "\t\tepoch_loss, epoch_acc\n",
    "\t))\n",
    "\n",
    "\t#Save the best model based on accuracy\n",
    "\tif epoch_acc > best_acc:\n",
    "\t\tbest_acc = epoch_acc\n",
    "\t\tbest_config = f\"{run}\"\n",
    "\t\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\t#Checkpoint\n",
    "\ttorch.save({\n",
    "\t\t\"epoch\": epoch,\n",
    "\t\t\"model_state_dict\": model.state_dict(),\n",
    "\t\t\"optimizer_state_dict\": optimizer.state_dict()\n",
    "\t}, \"./checkpoints/ckp.pt\")\n",
    "\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "\ttime_elapsed // 60, time_elapsed % 60\n",
    "))\n",
    "print('Best (so far) validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "print('-' * 10)\n",
    "print('### Final results ###\\n')\n",
    "print('Best validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "model.load_state_dict(best_model_wts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c67d8e0382c833d1d7ff2aaf3d36bcbf66720dd18abf32e81be90b4a114af6"
    }
   }
  },
  "language_info": {
   "name": "",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}