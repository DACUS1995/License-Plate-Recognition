{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms as T\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running training on [cuda]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import string\n",
    "from typing import Tuple\n",
    "import datetime\n",
    "import copy\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running training on [{device}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "WORKERS = 1\n",
    "EPOCHS = 10\n",
    "MAX_WORD_LENGTH = 10\n",
    "ALPHABET = string.ascii_letters + string.digits + \"_\" #blank char for CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, root_path, type=\"train\"):\n",
    "\t\tself.root_path = root_path\n",
    "\t\tself.type = type\n",
    "\t\tself.images_paths = list(pathlib.Path(self.root_path + \"./images\").glob('*.png'))\n",
    "\t\tself.transforms = {\n",
    "\t\t\t'train' : T.Compose([\n",
    "\t\t\t\tT.Resize((40,200)),\n",
    "\t\t\t\tT.RandomRotation(20),\n",
    "\t\t\t\tT.GaussianBlur(3),\n",
    "\t\t\t\tT.ToTensor()\n",
    "\t\t\t]),\n",
    "\t\t\t'valid' : T.Compose([\n",
    "\t\t\t\tT.ToTensor()\n",
    "\t\t\t])\n",
    "\t\t}\n",
    "\t\tglobal ALPHABET\n",
    "\t\tself.alphabet = ALPHABET\n",
    "\t\tself.alphabet_size = len(self.alphabet)\n",
    "\t\tprint(f\"Alphabet size: {self.alphabet_size}\")\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage_path = self.images_paths[idx]\n",
    "\t\tsample_name = str(image_path).split(os.sep)[-1].split(\".\")[0]\n",
    "\t\ttext_path = self.root_path + \"/transcripts/\" + str(int(sample_name) + 1) + \".txt\"\n",
    "\n",
    "\t\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\t\twith open(text_path) as f:\n",
    "\t\t\ttext = f.read()\n",
    "\n",
    "\t\timage = self.transforms[self.type](image)\n",
    "\t\ttext_tensor = self.wordToTensor(text)\n",
    "\t\treturn image, (text_tensor, len(text), text)\n",
    "\t\t\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images_paths)\n",
    "\n",
    "\tdef letterToIndex(self, letter):\n",
    "\t\treturn self.alphabet.find(letter)\n",
    "\n",
    "\tdef letterToTensor(self, letter):\n",
    "\t\ttensor = torch.zeros(1, n_letters)\n",
    "\t\ttensor[0][letterToIndex(letter)] = 1\n",
    "\t\treturn tensor\n",
    "\n",
    "\tdef wordToTensor(self, word):\n",
    "\t\t# tensor = torch.zeros(MAX_WORD_LENGTH, self.alphabet_size)\n",
    "\t\t# for li, letter in enumerate(word):\n",
    "\t\t# \ttensor[li][self.letterToIndex(letter)] = 1\n",
    "\n",
    "\t\ttensor = torch.zeros(MAX_WORD_LENGTH)\n",
    "\t\tfor li, letter in enumerate(word):\n",
    "\t\t\ttensor[li] = self.letterToIndex(letter)\n",
    "\t\treturn tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "\tdef __init__(self, run_config):\n",
    "\t\tself._training_dataset = None\n",
    "\t\tself._validation_dataset = None\n",
    "\t\tself._run_config = run_config\n",
    "\n",
    "\t\tself._load_datasets()\n",
    "\t\t\n",
    "\tdef _load_datasets(self):\n",
    "\t\tself._training_dataset = CustomDataset(\"dataset/training\")\n",
    "\t\tself._validation_dataset = CustomDataset(\"dataset/validation\")\n",
    "\n",
    "\tdef get_data_loaders(self) -> Tuple[DataLoader]:\n",
    "\t\treturn (\n",
    "\t\t\tDataLoader(self._training_dataset, batch_size=self._run_config[\"batch_size\"], shuffle=True, pin_memory=True, drop_last=True), \n",
    "\t\t\tDataLoader(self._validation_dataset, batch_size=self._run_config[\"batch_size\"], shuffle=True, pin_memory=True, drop_last=True)\n",
    "\t\t)\n",
    "\n",
    "\tdef get_datasets(self) -> Tuple[Dataset]:\n",
    "\t\treturn self._training_dataset, self._validation_dataset\n",
    "\n",
    "\tdef get_datasets_sizes(self) -> Tuple[int]:\n",
    "\t\treturn len(self._training_dataset), len(self._validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alphabet size: 63\nAlphabet size: 63\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(run_config = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"workers\": WORKERS\n",
    "})\n",
    "train_loader, validation_loader = data_handler.get_data_loaders()\n",
    "training_dataset_size, validation_dataset_size = data_handler.get_datasets_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscribeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TranscribeModel, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels=3, out_channels=16, kernel_size=2, padding=1),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2),\n",
    "\t\t\tnn.BatchNorm2d(16),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\t\t\tnn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, padding=1),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2),\n",
    "\t\t\tnn.BatchNorm2d(32),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True)\n",
    "\t\t)\n",
    "\n",
    "        self.rnn_block1 = nn.Sequential(\n",
    "            nn.LSTM(input_size=320, hidden_size=len(ALPHABET), num_layers=2, batch_first=True, bidirectional=True)\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv_block1(input)\n",
    "        # print(out.shape) #[128, 32, 10, 50]\n",
    "        out = out.permute([0, 3, 2, 1])\n",
    "        # (B, S, ) \n",
    "        out = out.reshape(out.size(0), out.size(1), -1)\n",
    "\n",
    "        out, (_, _) = self.rnn_block1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TranscribeModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (rnn_block1): Sequential(\n",
       "    (0): LSTM(320, 63, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=2)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "model = TranscribeModel()\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001, \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-08, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "loss_criterion = nn.CTCLoss(blank=len(ALPHABET)-1, zero_infinity=True, reduction=\"mean\")\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=10, epochs=EPOCHS, anneal_strategy='linear')\n",
    "\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorToWord(tensor):\n",
    "\ttensor = tensor[..., :63]\n",
    "\tindices = torch.argmax(tensor, dim=2).tolist()\n",
    "\t\n",
    "\twords = []\n",
    "\tfor batch_idx in range(BATCH_SIZE):\n",
    "\t\tcur_word_indices = indices[batch_idx]\n",
    "\t\tcur_word = []\n",
    "\t\tlast_letter = None\n",
    "\t\tfor idx in range(50):\n",
    "\t\t\tif ALPHABET[idx] == ALPHABET[-1]:\n",
    "\t\t\t\tlast_letter = None\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\tif last_letter == None or (last_letter is not None and last_letter != cur_word_indices[idx]):\n",
    "\t\t\t\t\tcur_word.append(ALPHABET[cur_word_indices[idx]])\n",
    "\t\t\t\t\tlast_letter = cur_word_indices[idx]\n",
    "\t\t\t\t\tcontinue\n",
    "\t\twords.append(\"\".join(cur_word))\n",
    "\treturn words\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([10,  2,  9,  5,  1,  9,  5,  1,  3,  1,  5,  8, 10,  9,  3,  5,  1,  3,\n         6, 10,  9,  3,  4,  3,  7,  6,  9,  1,  3,  3,  9,  4,  2,  6,  9, 10,\n         2,  8,  5,  5,  3,  3,  8,  6,  8,  6,  4, 10,  1,  1,  2,  3,  5,  4,\n         1,  5,  1,  6,  8,  8,  1,  5,  3,  4,  1,  5,  9,  3,  3,  4,  7,  3,\n         3,  7,  6,  9,  4,  2,  9,  9,  2,  6,  2,  4, 10, 10,  4,  9,  9, 10,\n         3,  3,  5,  6, 10,  6,  3,  4,  9, 10,  4,  2,  1,  9,  3,  3,  3,  2,\n         9,  5,  5,  2,  1,  5,  6,  3,  9,  4,  3,  3,  1,  2,  6,  8, 10,  4,\n        10,  2])\n"
     ]
    }
   ],
   "source": [
    "image, (label_tensor, text_length, label_text) = iter(train_loader).next()\n",
    "image, label_tensor = image.to(device), label_tensor.to(device)\n",
    "output = model(image)\n",
    "print(text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 126])"
      ]
     },
     "metadata": {},
     "execution_count": 77
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['M8tDd8al12', 'NF', '4AtoxGqR8', 'OrfVq', '3', 'devi1apKs', 'WpfTC', 'k', 'PGw', 'b', 'KgBGD', 'uk5l06CJ', 'MO9WV6VHVz', 'YtydeKiFE', 'fFN', '1cCBe', 'd', 'iQS', 'nmweTu', 'HDZoo256vM', 'u1OFnqGt3', 'hfr', 'hCAT', 'vzb', '5DhZbxQ', 'OLo7MM', 'vyqUhKOTs', 'U', '5Ls', 'oO1', 'nsjgkiVqm', '89zR', '6X', 'ivJnu6', 'S165Ik8e3', '5ISKlSZxVP', 'KH', '1ZVl5sJj', 'nbXAZ', 'Zp6kj', 'AcR', 'qsY', 'QbuH65c1', 'hC9DqX', 'uGIacJAh', '4SSQ7Y', 'GhcM', '9KkmrksTMW', 'L', 'E', 'ZB', 'YBg', 'RG19h', 'phRQ', 'd', 'd3whZ', 'O', 'mPqsa3', 'mMrvpgTF', 'UKdbpgLV', 'J', 'zbrUm', 'lmh', 'Lx7d', '3', 'bCd5Y', 'CJKrPXvvm', 'sKv', 'QXz', 'jBF0', 'xJYlLAR', 'hmO', 'Yv0', 'rFKPBUu', 'JyoeBe', 'T2Y7kUe9X', 'a83l', 'Ie', 'KXXIH8WUO', '2uNR1wdRj', 'Wp', 'yU9Gn8', 'xO', 'VPB5', 'XYrg0rQhvV', 'vTecj9NOHw', 'T36O', '4Bug3btdx', 'exNo9iUvY', 'H4CcfwUAoe', 'tzp', '0rB', 'AxQts', 'FICXlj', 'yZMYc2F7pJ', 'NXvgEX', 'kvX', '03pP', 'eIE4SgIl9', 'Wmh5V9QkuI', 'hgkq', 'pI', 'I', 'aOmkLfFnQ', 'JKe', 'UOp', 'cxM', '96', 'KTmfSXJYV', 'TRDjv', 'r2Epp', '6c', 'l', 'ERICK', 'Vo5QoZ', 'DdQ', 'ham326GaX', 'rGbL', 'DRN', 'XdQ', 'R', 'wo', '8L2AtR', 'q9tQEfjT', 'Ecsyu3GhKq', 'rL3o', '07yiKb7Tr4', 'js']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_']"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "output.shape\n",
    "out = output[..., :63]\n",
    "# torch.argmax(out, dim=2)\n",
    "print(label_text)\n",
    "\n",
    "tensorToWord(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/10\n----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [1] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab27eb676cc544f5836f9c0b9e5454a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.6514\n",
      "Evaluation step => Loss: -1.4052\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [2] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9221ecefe4f142b585a8a2fde48ccda7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.6601\n",
      "Evaluation step => Loss: -1.4117\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [3] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e57ff9720f54c93886274bf6a596c7c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.6730\n",
      "Evaluation step => Loss: -1.3961\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [4] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "827aec8faa8e4a6ca7aaa01cacb83096"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.6825\n",
      "Evaluation step => Loss: -1.4145\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [5] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05785a515b074decad62ab7ddc5f724f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.6968\n",
      "Evaluation step => Loss: -1.4589\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [6] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a76d2a27b8f4923bc2d6784e463435d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.7011\n",
      "Evaluation step => Loss: -1.4297\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [7] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e255ebe35267422b98c2a9c0f5fb67ac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.7078\n",
      "Evaluation step => Loss: -1.4200\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [8] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f492d50949e4b98ad24028191005980"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.7076\n",
      "Evaluation step => Loss: -1.4376\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [9] progress', max=48.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d641c157ea394768af33ea8fb3246813"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.7076\n",
      "Evaluation step => Loss: -1.4351\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch [10] progress', max=48.0, style=ProgressStyle(descr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a072f6be3fb41268ce7b05c80dddb7b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training step => Loss: -1.7087\n",
      "Evaluation step => Loss: -1.4185\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-7ecd8028cde4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msince\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m print('Training complete in {:.0f}m {:.0f}s'.format(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\tprint('Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "\tprint('-' * 10)\n",
    "\n",
    "\t########### Training step ###########\n",
    "\tmodel = model.train()\n",
    "\ttraining_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\t\t\t\n",
    "\tfor i, data in enumerate(tqdm(train_loader, desc=f\"Epoch [{epoch + 1}] progress\")):\n",
    "\n",
    "\t\tx_batch, (label_batch, label_length, label_text) = data\n",
    "\t\tx_batch, label_batch, label_length = x_batch.to(device), label_batch.to(device), label_length.to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(x_batch)\n",
    "\t\toutputs_permuted = outputs.permute((1, 0, 2))\n",
    "\n",
    "\t\tloss = loss_criterion(outputs_permuted, label_batch, torch.full((BATCH_SIZE,), MAX_WORD_LENGTH).to(device), label_length)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# statistics\n",
    "\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\trunning_corrects += torch.tensor([0])\n",
    "\t\t# running_corrects += torch.sum(preds == label_text)\n",
    "\t\ttraining_loss.append(loss.item())\n",
    "\n",
    "\tepoch_loss = running_loss / training_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / training_dataset_size\n",
    "\n",
    "\t# tensorboard logging\n",
    "\twriter.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "\n",
    "\tprint('Training step => Loss: {:.4f}'.format(\n",
    "\t\tepoch_loss\n",
    "\t))\n",
    "\n",
    "\tscheduler.step()\n",
    "\n",
    "\n",
    "\t########### Validation step ###########\n",
    "\tmodel = model.eval()\n",
    "\tvalidation_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\n",
    "\tfor i, data in enumerate(validation_loader):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tx_batch, (label_batch, label_length, label_text) = data\n",
    "\t\t\tx_batch, label_batch, label_length = x_batch.to(device), label_batch.to(device), label_length.to(device)\n",
    "\n",
    "\t\t\toutputs = model(x_batch)\n",
    "\t\t\toutputs = outputs.permute((1, 0, 2))\n",
    "\t\t\tloss = loss_criterion(outputs, label_batch, torch.full((BATCH_SIZE,), MAX_WORD_LENGTH).to(device), label_length)\n",
    "\n",
    "\t\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\t\trunning_corrects += torch.tensor([0])\n",
    "\t\t\t# running_corrects += torch.sum(preds == label_batch.detach())\n",
    "\t\t\tvalidation_loss.append(loss.item())\n",
    "\t\t\t\n",
    "\tepoch_loss = running_loss / validation_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / validation_dataset_size\n",
    "\n",
    "\t# tensorboard logging\n",
    "\twriter.add_scalar(\"Loss/validation\", epoch_loss, epoch)\n",
    "\n",
    "\tprint('Evaluation step => Loss: {:.4f}'.format(\n",
    "\t\tepoch_loss\n",
    "\t))\n",
    "\tbest_acc = 0\n",
    "\t#Save the best model based on accuracy\n",
    "\tif True:\n",
    "\t\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\t#Checkpoint\n",
    "\ttorch.save({\n",
    "\t\t\"epoch\": epoch,\n",
    "\t\t\"model_state_dict\": model.state_dict(),\n",
    "\t\t\"optimizer_state_dict\": optimizer.state_dict()\n",
    "\t}, \"./checkpoints/ckp.pt\")\n",
    "\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "\ttime_elapsed // 60, time_elapsed % 60\n",
    "))\n",
    "print('Best (so far) validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "print('-' * 10)\n",
    "print('### Final results ###\\n')\n",
    "print('Best validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "97c67d8e0382c833d1d7ff2aaf3d36bcbf66720dd18abf32e81be90b4a114af6"
    }
   }
  },
  "language_info": {
   "name": "",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}